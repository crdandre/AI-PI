# Configuration for language models used in different tasks.
# Each task can specify:
#   - model_name: The model to use (required)
#   - temperature: Sampling temperature (default: 0.9)
#   - predictor_type: DSPy predictor class to use (default: "Predict")
#   - api_base: Optional API endpoint override
#   - api_key: Optional API key override
#   - max_tokens: Optional maximum tokens limit
#
# Note: ensure tasks with input images use multimodal models like llama3.2-vision

default:
  model_name: "openrouter/deepseek/deepseek-chat"
  temperature: 0.9
  predictor_type: "ChainOfThought"

summarization:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "Predict"

document_review:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "ChainOfThought"

section_identification:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "ChainOfThought"

section_review:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "ChainOfThought"

image_caption_extraction:
  model_name: "ollama_chat/llama3.2-vision:latest"
  predictor_type: "Predict"

caption_analysis:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "ChainOfThought"

caption_combination:
  model_name: "ollama_chat/llama3.2-vision:latest"
  predictor_type: "Predict"

markdown_segmentation:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "Predict"

storm_writer:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "ChainOfThought"

storm_questions:
  model_name: "openrouter/deepseek/deepseek-chat"
  predictor_type: "ChainOfThought"