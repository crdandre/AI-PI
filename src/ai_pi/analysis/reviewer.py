"""
This file contains the code for an agent which creates suggestions for comments 
and revisions, simulating the style of feedback a PI would give when reviewing a paper.
"""
import dspy
import json
import logging
from ..lm_config import get_lm_for_task, LMConfig
from dataclasses import dataclass
from typing import List, Dict, Optional

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReviewItem(dspy.Signature):
    """Individual review item structure"""
    match_text = dspy.OutputField(desc="Exact text to match")
    comment = dspy.OutputField(desc="Review comment")
    revision = dspy.OutputField(desc="Complete revised text")

class ReviewerSignature(dspy.Signature):
    """Generate high-level scientific review feedback for a section of text.
    
    Apply careful stepwise thinking to each piece of this task:
    
    You are a senior academic reviewer with expertise in providing strategic,
    high-level manuscript feedback.
    
    First, consider the section type and its expected scope. Each section has specific
    requirements and constraints:
    
    Introduction:
    - Focus on: Problem framing, literature coverage, hypothesis/objective clarity
    - Avoid: Detailed methodological critiques or results interpretation
    - Key questions: Is the research gap clear? Are objectives well-justified?
    
    Methods:
    - Focus on: Reproducibility, technical soundness, procedure completeness
    - Avoid: Discussing implications or broader impact
    - Key questions: Could another researcher replicate this? Are choices justified?
    
    Results:
    - Focus on: Data presentation, statistical reporting, factual outcomes
    - Avoid: Detailed interpretation or speculation about mechanisms
    - Key questions: Are the findings clearly presented? Is statistical reporting complete?
    
    Discussion:
    - Focus on: Result interpretation, context in literature, limitations
    - Avoid: Introducing new results or detailed methods
    - Key questions: Are conclusions supported by the results? Are limitations addressed?
    
    Abstract:
    - Focus on: Balance, completeness, alignment with paper
    - Avoid: Technical details or extensive background
    - Key questions: Are key findings represented? Is scope appropriate?
    
    For each potential review item, ensure your feedback aligns with the section's
    purpose. Redirect misplaced content to appropriate sections rather than requesting
    detailed elaboration in the wrong section.
    
    Focus on scientific merit, methodology soundness, and theoretical contributions rather than
    minor writing issues. After generating feedback, reflect on whether your comments address
    substantive scientific concerns.
    
    For each potential review item, reflect on the structure below and contemplate whether each potential
    review item warrants a comment, a revision, both, or neither. If either of "comment" or "revision" fields
    are not to be included based on your decision of the best way to provide feedback, fill the field with "N/A".
    
    Consider whether the review item would be a part of the high-level overview, which
    is generated by this code. If so, IGNORE IT!
    
    I stress this - the goal is to make the most useful feedback for the writer who submitted their manuscript
    for your review. This should be done without spamming comments and revisions, but judging what is crucial or
    important to the scientific value of the manuscript and it's accessibility/clarity. Rarely (but sometimes) this
    will mean no feedback at all, but in almost every case there will be at least something to comment on or compliment.
    
    That is, if something is great, you can reinforce that too! Only if it's truly exceptional work
    (you've seen it all as a senior professor).
    
    For example:
    Input:
        section_text: "The model predicted curve progression with an average error of 5 degrees."
        section_type: "Results"
        context: "Paper about scoliosis modeling"
    Output:
        initial_analysis: "The results lack scientific rigor in three areas: (1) no statistical validation, 
            (2) missing comparison with state-of-the-art methods, (3) no discussion of clinical significance."
        reflection: "My feedback appropriately focuses on core scientific issues rather than superficial edits. 
            The comments address methodology, validation, and clinical relevance."
        review_items: [
            {
                "match_text": "The model predicted curve progression with an average error of 5 degrees.",
                "comment": "The results require statistical validation (e.g., confidence intervals) and 
                    discussion of clinical significance. How does this error rate impact treatment decisions?",
                "revision": "The model predicted curve progression with an average error of 5° (95% CI: 3.2-6.8°). 
                    This accuracy level is clinically significant as it falls within the threshold needed for 
                    reliable treatment planning (< 7°), based on established clinical guidelines."
            }
        ]
    """
    section_text = dspy.InputField(desc="The text content of the section to review")
    section_type = dspy.InputField(desc="The type of section (e.g., Methods, Results)")
    context = dspy.InputField(desc="Additional context about the paper")
    initial_analysis = dspy.OutputField(desc="Initial high-level analysis of scientific concerns")
    reflection = dspy.OutputField(desc="Self-reflection on whether feedback addresses core scientific issues")
    review_items = dspy.OutputField(desc="List of review items in JSON format", format=list[ReviewItem])
    conceptual_framework = dspy.OutputField(desc="Analysis of how the section fits into broader theoretical framework")
    methodology_assessment = dspy.OutputField(desc="Detailed assessment of methodological choices and alternatives")
    impact_analysis = dspy.OutputField(desc="Analysis of potential impact and implications")
    cross_references = dspy.OutputField(desc="Connections to other sections and related work")
    knowledge_gaps = dspy.OutputField(desc="Identification of remaining knowledge gaps")

class FinalReviewSignature(dspy.Signature):
    """Generate final synthesized review across all sections."""
    section_analyses = dspy.InputField(desc="List of section-level analyses")
    paper_context = dspy.InputField(desc="Overall paper context")
    overall_assessment = dspy.OutputField(desc="Synthesized scientific assessment of the entire paper")
    key_strengths = dspy.OutputField(desc="Major scientific strengths identified")
    key_weaknesses = dspy.OutputField(desc="Major scientific weaknesses identified")
    recommendations = dspy.OutputField(desc="High-priority recommendations for improvement")

@dataclass
class ScoringMetrics:
    """Standardized scoring metrics for paper evaluation"""
    clarity: float
    methodology: float
    novelty: float
    impact: float
    presentation: float
    literature_integration: float
    
    def to_dict(self) -> dict:
        return {k: float(v) for k, v in self.__dict__.items()}

@dataclass
class SectionAnalysis:
    """Detailed analysis of a paper section"""
    section_type: str
    content_summary: str
    role_in_paper: str
    key_points: List[str]
    dependencies: List[str]
    supports: List[str]
    metrics: ScoringMetrics

class ExtractComponent(dspy.Signature):
    """Generic signature for extracting paper components"""
    sections = dspy.InputField(desc="List of paper sections")
    target = dspy.InputField(desc="Component to extract (e.g., 'research_problem', 'hypotheses')")
    extracted = dspy.OutputField(desc="Extracted component")

class AnalyzeComponent(dspy.Signature):
    """Generic signature for analyzing paper components"""
    content = dspy.InputField(desc="Content to analyze")
    analysis_type = dspy.InputField(desc="Type of analysis (e.g., 'sections', 'metrics')")
    analysis = dspy.OutputField(desc="Analysis results")

class GenerateComponent(dspy.Signature):
    """Generic signature for generating review components"""
    content = dspy.InputField(desc="Content to analyze")
    component_type = dspy.InputField(desc="Type to generate (e.g., 'strengths', 'suggestions')")
    generated = dspy.OutputField(desc="Generated content")

class ReviewMetrics(dspy.Signature):
    """Signature for scoring paper sections"""
    content = dspy.InputField(desc="Section content")
    metrics = dspy.OutputField(desc="Metrics scores", format=ScoringMetrics)

class Reviewer(dspy.Module):
    """Reviews individual sections with awareness of full paper context"""
    
    def __init__(self, 
                 lm: LMConfig = None,
                 verbose: bool = False):
        super().__init__()
        self.lm = get_lm_for_task("review", lm)
        self.paper_knowledge = None
        
        # Simplify to use Predict directly
        self.reviewer = dspy.Predict(ReviewerSignature)
        self.final_reviewer = dspy.Predict(FinalReviewSignature)
        self.extractor = dspy.Predict(ExtractComponent)
        self.analyzer = dspy.Predict(AnalyzeComponent)
        self.generator = dspy.Predict(GenerateComponent)
        self.metrics_scorer = dspy.Predict(ReviewMetrics)
        
        # Add class type logging
        if verbose:
            print(f"Reviewer type: {type(self.reviewer).__name__}")
            print(f"Final reviewer type: {type(self.final_reviewer).__name__}")
        
        # Add example as a class attribute if needed for reference
        self.example_input = {
            "section_text": "The model predicted curve progression with an average error of 5 degrees.",
            "section_type": "Results",
            "context": "Paper about scoliosis modeling"
        }
        self.example_output = {
            "reasoning": "This result statement needs more specificity about the error metric and comparison to existing methods.",
            "review_items": [
                {
                    "match_text": "The model predicted curve progression with an average error of 5 degrees.",
                    "comment": "The error metric should be specified (RMSE, MAE?) and compared to previous work.",
                    "revision": "The model predicted curve progression with a root mean square error of 5 degrees, improving upon previous methods which reported errors of 8-10 degrees."
                }
            ]
        }
        
        self.verbose = verbose
        
        # Add section-specific validation criteria as class attribute
        self.section_criteria = {
            'introduction': {
                'focus': ['research gap', 'objectives', 'background', 'hypothesis', 'problem framing', 'literature'],
                'avoid': ['detailed method', 'result interpretation', 'technical detail']
            },
            'methods': {
                'focus': ['procedure', 'protocol', 'technique', 'reproducibility', 'technical soundness'],
                'avoid': ['implication', 'broader impact', 'result interpretation']
            },
            'results': {
                'focus': ['outcome', 'statistical', 'measurement', 'data', 'factual findings'],
                'avoid': ['mechanism speculation', 'detailed interpretation', 'broader implications']
            },
            'discussion': {
                'focus': ['interpretation', 'implication', 'limitation', 'context', 'literature comparison'],
                'avoid': ['new result', 'detailed method', 'raw data']
            },
            'abstract': {
                'focus': ['summary', 'overview', 'key finding', 'balance'],
                'avoid': ['technical detail', 'extensive background', 'detailed method']
            }
        }
        
        # Add review quality criteria
        self.review_quality_criteria = {
            'depth': [
                'theoretical foundation',
                'methodological rationale',
                'alternative approaches',
                'limitations analysis',
                'broader implications'
            ],
            'integration': [
                'cross-section connections',
                'literature synthesis',
                'field positioning',
                'future directions'
            ],
            'specificity': [
                'concrete examples',
                'detailed critiques',
                'actionable suggestions',
                'evidence-based assessment'
            ],
            'context_awareness': [
                'cross_references',
                'dependency_handling',
                'support_validation'
            ]
        }

    def review_document(self, document_json: dict) -> dict:
        """Enhanced review process that maintains global context"""
        try:
            logger.info("Starting document review...")
            
            # 1. Build paper-wide understanding first
            logger.info("Building paper knowledge...")
            self.paper_knowledge = self._build_paper_knowledge(document_json)
            
            # 2. Generate main review
            logger.info("Generating main review...")
            main_review = self._generate_main_review()
            
            # 3. Review individual sections with full context
            logger.info("Reviewing individual sections...")
            section_reviews = []
            for section in document_json.get('sections', []):
                logger.info(f"Reviewing section: {section['section_type']}")
                review = self.review_section({
                    'text': section.get('content', ''),
                    'section_type': section.get('section_type', '')
                }, paper_context=self.paper_knowledge)
                section_reviews.append({
                    'section_type': section['section_type'],
                    'review': review
                })
            
            # 4. Compile final document with single metrics output
            document_json['reviews'] = {
                'main_review': main_review,
                'section_reviews': section_reviews,
                'metrics': {
                    'clarity': self.paper_knowledge.get('metrics', ScoringMetrics(0,0,0,0,0,0)).clarity,
                    'methodology': self.paper_knowledge.get('metrics', ScoringMetrics(0,0,0,0,0,0)).methodology,
                    'novelty': self.paper_knowledge.get('metrics', ScoringMetrics(0,0,0,0,0,0)).novelty,
                    'impact': self.paper_knowledge.get('metrics', ScoringMetrics(0,0,0,0,0,0)).impact,
                    'presentation': self.paper_knowledge.get('metrics', ScoringMetrics(0,0,0,0,0,0)).presentation,
                    'literature_integration': self.paper_knowledge.get('metrics', ScoringMetrics(0,0,0,0,0,0)).literature_integration
                }
            }
            
            return document_json
            
        except Exception as e:
            logger.error(f"Error reviewing document: {str(e)}", exc_info=True)
            raise

    def _build_paper_knowledge(self, document_json: dict) -> dict:
        """Builds comprehensive paper understanding"""
        try:
            sections = document_json.get('sections', [])
            extract_targets = ['research_problem', 'hypotheses', 'key_methods', 
                             'main_findings', 'limitations']
            
            with dspy.context(lm=self.lm):
                # Simplified direct predictions
                components = {
                    target: self.extractor(sections=sections, target=target).extracted
                    for target in extract_targets
                }
                
                components.update({
                    'section_analyses': self.analyzer(content=sections, analysis_type='sections').analysis,
                    'metrics': self.analyzer(content=sections, analysis_type='metrics').analysis
                })
            
            return components
                
        except Exception as e:
            logger.error(f"Error building paper knowledge: {str(e)}")
            return {}

    def review_section(self, section: dict, paper_context: dict = None) -> dict:
        """Reviews section using DSPy predictors"""
        with dspy.context(lm=self.lm):
            metrics = self.metrics_scorer(
                content=section['text']
            ).metrics
            
            analysis = self.analyzer(
                content=section,
                analysis_type='section_review'
            ).analysis
            
            return {
                'metrics': metrics,
                'review': analysis
            }

    def _generate_main_review(self) -> dict:
        """Generates the main review using DSPy predictors"""
        with dspy.context(lm=self.lm):
            return {
                'key_strengths': self.generator(
                    content=self.paper_knowledge,
                    component_type='strengths'
                ).generated,
                'key_weaknesses': self.generator(
                    content=self.paper_knowledge,
                    component_type='weaknesses'
                ).generated,
                'global_suggestions': self.generator(
                    content=self.paper_knowledge,
                    component_type='suggestions'
                ).generated
            }

    def forward(self, section_text: str, section_type: str, paper_context: dict) -> dspy.Prediction:
        """Enhanced review process with quality validation."""
        if not section_text or not section_type:
            logger.warning("Empty input received")
            return self._create_empty_review()

        try:
            # Simplified direct prediction
            result = self.reviewer(
                section_text=str(section_text),
                section_type=str(section_type),
                context=str(paper_context.get('paper_summary', ''))
            )
            
            # Validate and filter review items based on section type
            items = json.loads(result.review_items) if isinstance(result.review_items, str) else result.review_items
            filtered_items = self._validate_section_specific_feedback(items, section_type.lower())
            
            review_data = {
                'match_strings': [],
                'comments': [],
                'revisions': [],
                'initial_analysis': result.initial_analysis,
                'reflection': result.reflection
            }
            
            for item in filtered_items:
                review_data['match_strings'].append(item.get('match_text', ''))
                review_data['comments'].append(item.get('comment', 'N/A'))
                review_data['revisions'].append(item.get('revision', 'N/A'))
            
            # Add quality validation step
            meets_criteria, suggestions = self._validate_review_quality(filtered_items)
            
            if not meets_criteria:
                # Request additional review iteration with quality improvements
                with dspy.settings.context(lm=self.lm):
                    improved_result = self.reviewer(
                        section_text=section_text,
                        section_type=section_type,
                        context=f"{paper_context.get('paper_summary', '')} \n\nImprovement needed: {', '.join(suggestions)}"
                    )
                    filtered_items = self._validate_section_specific_feedback(
                        improved_result.review_items, 
                        section_type.lower()
                    )
            
            return dspy.Prediction(review=review_data)
            
        except Exception as e:
            logger.error(f"Error in forward method: {str(e)}")
            return self._create_empty_review()

    def _validate_section_specific_feedback(self, review_items: list, section_type: str) -> list:
        """Filter review items based on section-specific criteria."""
        if section_type not in self.section_criteria:
            logger.warning(f"Unknown section type: {section_type}")
            return review_items
        
        criteria = self.section_criteria[section_type]
        filtered_items = []
        
        for item in review_items:
            comment = item.get('comment', '').lower()
            revision = item.get('revision', '').lower()
            
            # Check if comment contains any terms to avoid
            should_avoid = any(avoid_term in comment or avoid_term in revision 
                             for avoid_term in criteria['avoid'])
            
            # Check if comment contains focus terms
            has_focus = any(focus_term in comment or focus_term in revision 
                           for focus_term in criteria['focus'])
            
            if has_focus and not should_avoid:
                filtered_items.append(item)
            else:
                logger.debug(f"Filtered out review item for {section_type}: {item}")
        
        return filtered_items

    # #TODO: add validation to increase depth and usefulness of feedback
    # # and avoid superficial comments (this is like an internal reflection loop)
    # # to improve itself
    # def _validate_scientific_depth(self, result) -> bool:
    #     return True

    def _create_empty_review(self) -> dspy.Prediction:
        return dspy.Prediction(review={'match_strings': [], 'comments': [], 'revisions': []})

    def compile_final_review(self, section_analyses: list, paper_context: dict) -> dict:
        """Compile a final high-level review synthesizing all sections."""
        try:
            with dspy.settings.context(lm=self.lm):
                final_review = self.final_reviewer(
                    section_analyses=section_analyses,
                    paper_context=paper_context
                )
                
                return {
                    'overall_assessment': str(final_review.overall_assessment),
                    'key_strengths': [str(s) for s in (final_review.key_strengths if isinstance(final_review.key_strengths, list) else [final_review.key_strengths])],
                    'key_weaknesses': [str(w) for w in (final_review.key_weaknesses if isinstance(final_review.key_weaknesses, list) else [final_review.key_weaknesses])],
                    'recommendations': [str(r) for r in (final_review.recommendations if isinstance(final_review.recommendations, list) else [final_review.recommendations])],
                }
        except Exception as e:
            logger.error(f"Error compiling final review: {str(e)}")
            return {
                'overall_assessment': "Error generating final review",
                'key_strengths': [],
                'key_weaknesses': [],
                'recommendations': [],
            }

    def _validate_review_quality(self, review_items: list) -> tuple[bool, list]:
        """Validate review quality against defined criteria."""
        quality_scores = {
            'depth': 0,
            'integration': 0,
            'specificity': 0
        }
        
        improvement_suggestions = []
        
        # Analyze each review item against quality criteria
        for item in review_items:
            comment = item.get('comment', '').lower()
            
            # Check depth
            depth_score = sum(1 for term in self.review_quality_criteria['depth'] 
                            if term in comment)
            quality_scores['depth'] += depth_score
            
            # Check integration
            integration_score = sum(1 for term in self.review_quality_criteria['integration'] 
                                 if term in comment)
            quality_scores['integration'] += integration_score
            
            # Check specificity
            specificity_score = sum(1 for term in self.review_quality_criteria['specificity'] 
                                  if term in comment)
            quality_scores['specificity'] += specificity_score
        
        # Generate improvement suggestions if scores are low
        if quality_scores['depth'] < len(review_items):
            improvement_suggestions.append("Deepen theoretical analysis and methodological critique")
        if quality_scores['integration'] < len(review_items):
            improvement_suggestions.append("Strengthen connections across sections and to broader literature")
        if quality_scores['specificity'] < len(review_items):
            improvement_suggestions.append("Provide more concrete examples and actionable suggestions")
        
        # Return True if all quality criteria meet minimum thresholds
        meets_criteria = all(score >= len(review_items) * 0.5 
                           for score in quality_scores.values())
        
        return meets_criteria, improvement_suggestions

    def _identify_key_strengths(self) -> List[str]:
        """Implement missing method"""
        return []

    def _identify_key_weaknesses(self) -> List[str]:
        """Implement missing method"""
        return []

    def _generate_global_suggestions(self) -> List[str]:
        """Implement missing method"""
        return []

    def _generate_section_specific_feedback(self) -> Dict:
        """Implement missing method"""
        return {}


if __name__ == "__main__":
    import os
    import json
    
    reviewer = Reviewer(verbose=True)
    
    # Load and process sample document
    with open("processed_documents/ScolioticFEPaper_v7_20250107_011921/ScolioticFEPaper_v7_reviewed.json", "r") as f:
        document = json.load(f)
    
    # Review entire document
    reviewed_document = reviewer.review_document(document)
    
    # Print results
    print(json.dumps(reviewed_document, indent=4))
