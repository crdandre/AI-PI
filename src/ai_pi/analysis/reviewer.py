"""
This file contains the code for an agent which creates suggestions for comments 
and revisions, simulating the style of feedback a PI would give when reviewing a paper.
"""
import dspy
import json
import logging
import ast

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReviewItem(dspy.Signature):
    """Individual review item structure"""
    match_text = dspy.OutputField(desc="Exact text to match")
    comment = dspy.OutputField(desc="Review comment")
    revision = dspy.OutputField(desc="Complete revised text")

class ReviewerSignature(dspy.Signature):
    """Generate high-level scientific review feedback for a section of text.
    
    Apply careful stepwise thinking to each piece of this task:
    
    You are a senior academic reviewer with expertise in providing strategic,
    high-level manuscript feedback.
    
    First, consider the section type and its expected scope. Each section has specific
    requirements and constraints:
    
    Introduction:
    - Focus on: Problem framing, literature coverage, hypothesis/objective clarity
    - Avoid: Detailed methodological critiques or results interpretation
    - Key questions: Is the research gap clear? Are objectives well-justified?
    
    Methods:
    - Focus on: Reproducibility, technical soundness, procedure completeness
    - Avoid: Discussing implications or broader impact
    - Key questions: Could another researcher replicate this? Are choices justified?
    
    Results:
    - Focus on: Data presentation, statistical reporting, factual outcomes
    - Avoid: Detailed interpretation or speculation about mechanisms
    - Key questions: Are the findings clearly presented? Is statistical reporting complete?
    
    Discussion:
    - Focus on: Result interpretation, context in literature, limitations
    - Avoid: Introducing new results or detailed methods
    - Key questions: Are conclusions supported by the results? Are limitations addressed?
    
    Abstract:
    - Focus on: Balance, completeness, alignment with paper
    - Avoid: Technical details or extensive background
    - Key questions: Are key findings represented? Is scope appropriate?
    
    For each potential review item, ensure your feedback aligns with the section's
    purpose. Redirect misplaced content to appropriate sections rather than requesting
    detailed elaboration in the wrong section.
    
    Focus on scientific merit, methodology soundness, and theoretical contributions rather than
    minor writing issues. After generating feedback, reflect on whether your comments address
    substantive scientific concerns.
    
    For each potential review item, reflect on the structure below and contemplate whether each potential
    review item warrants a comment, a revision, both, or neither. If either of "comment" or "revision" fields
    are not to be included based on your decision of the best way to provide feedback, fill the field with "N/A".
    
    Consider whether the review item would be a part of the high-level overview, which
    is generated by this code. If so, IGNORE IT!
    
    I stress this - the goal is to make the most useful feedback for the writer who submitted their manuscript
    for your review. This should be done without spamming comments and revisions, but judging what is crucial or
    important to the scientific value of the manuscript and it's accessibility/clarity. Rarely (but sometimes) this
    will mean no feedback at all, but in almost every case there will be at least something to comment on or compliment.
    
    That is, if something is great, you can reinforce that too! Only if it's truly exceptional work
    (you've seen it all as a senior professor).
    
    For example:
    Input:
        section_text: "The model predicted curve progression with an average error of 5 degrees."
        section_type: "Results"
        context: "Paper about scoliosis modeling"
    Output:
        initial_analysis: "The results lack scientific rigor in three areas: (1) no statistical validation, 
            (2) missing comparison with state-of-the-art methods, (3) no discussion of clinical significance."
        reflection: "My feedback appropriately focuses on core scientific issues rather than superficial edits. 
            The comments address methodology, validation, and clinical relevance."
        review_items: [
            {
                "match_text": "The model predicted curve progression with an average error of 5 degrees.",
                "comment": "The results require statistical validation (e.g., confidence intervals) and 
                    discussion of clinical significance. How does this error rate impact treatment decisions?",
                "revision": "The model predicted curve progression with an average error of 5° (95% CI: 3.2-6.8°). 
                    This accuracy level is clinically significant as it falls within the threshold needed for 
                    reliable treatment planning (< 7°), based on established clinical guidelines."
            }
        ]
    """
    section_text = dspy.InputField(desc="The text content of the section to review")
    section_type = dspy.InputField(desc="The type of section (e.g., Methods, Results)")
    context = dspy.InputField(desc="Additional context about the paper")
    initial_analysis = dspy.OutputField(desc="Initial high-level analysis of scientific concerns")
    reflection = dspy.OutputField(desc="Self-reflection on whether feedback addresses core scientific issues")
    review_items = dspy.OutputField(desc="List of review items in JSON format", format=list[ReviewItem])

class FinalReviewSignature(dspy.Signature):
    """Generate final synthesized review across all sections."""
    section_analyses = dspy.InputField(desc="List of section-level analyses")
    paper_context = dspy.InputField(desc="Overall paper context")
    overall_assessment = dspy.OutputField(desc="Synthesized scientific assessment of the entire paper")
    key_strengths = dspy.OutputField(desc="Major scientific strengths identified")
    key_weaknesses = dspy.OutputField(desc="Major scientific weaknesses identified")
    recommendations = dspy.OutputField(desc="High-priority recommendations for improvement")

class Reviewer(dspy.Module):
    """Reviews individual sections with awareness of full paper context"""
    
    def __init__(self, 
                 engine: dspy.dsp.LM, 
                 reviewer_class: str = "ReAct",
                 verbose: bool = False):
        super().__init__()
        
        # Initialize predict tool with proper signature
        predict_tool = dspy.Predict(ReviewerSignature)
        
        # Map string names to reviewer classes
        reviewer_classes = {
            "ReAct": lambda sig: dspy.ReAct(
                signature=sig,
                tools=[predict_tool],
                max_iters=5
            ),
            "ChainOfThought": dspy.ChainOfThought,
            "Predict": dspy.Predict
        }
        
        # Get reviewer class from mapping, default to Predict if not found
        ReviewerClass = reviewer_classes.get(reviewer_class, dspy.Predict)
        logger.info(f"Using reviewer class: {ReviewerClass.__name__}")
        
        # Initialize with signature
        self.reviewer = ReviewerClass(ReviewerSignature)
        self.final_reviewer = ReviewerClass(FinalReviewSignature)
        
        # Add class type logging
        if verbose:
            print(f"Reviewer type: {type(self.reviewer).__name__}")
            print(f"Final reviewer type: {type(self.final_reviewer).__name__}")
        
        # Add example as a class attribute if needed for reference
        self.example_input = {
            "section_text": "The model predicted curve progression with an average error of 5 degrees.",
            "section_type": "Results",
            "context": "Paper about scoliosis modeling"
        }
        self.example_output = {
            "reasoning": "This result statement needs more specificity about the error metric and comparison to existing methods.",
            "review_items": [
                {
                    "match_text": "The model predicted curve progression with an average error of 5 degrees.",
                    "comment": "The error metric should be specified (RMSE, MAE?) and compared to previous work.",
                    "revision": "The model predicted curve progression with a root mean square error of 5 degrees, improving upon previous methods which reported errors of 8-10 degrees."
                }
            ]
        }
        
        self.engine = engine
        self.verbose = verbose
        
        # Add section-specific validation criteria as class attribute
        self.section_criteria = {
            'introduction': {
                'focus': ['research gap', 'objectives', 'background', 'hypothesis', 'problem framing', 'literature'],
                'avoid': ['detailed method', 'result interpretation', 'technical detail']
            },
            'methods': {
                'focus': ['procedure', 'protocol', 'technique', 'reproducibility', 'technical soundness'],
                'avoid': ['implication', 'broader impact', 'result interpretation']
            },
            'results': {
                'focus': ['outcome', 'statistical', 'measurement', 'data', 'factual findings'],
                'avoid': ['mechanism speculation', 'detailed interpretation', 'broader implications']
            },
            'discussion': {
                'focus': ['interpretation', 'implication', 'limitation', 'context', 'literature comparison'],
                'avoid': ['new result', 'detailed method', 'raw data']
            },
            'abstract': {
                'focus': ['summary', 'overview', 'key finding', 'balance'],
                'avoid': ['technical detail', 'extensive background', 'detailed method']
            }
        }

    def review_document(self, document_json: dict) -> dict:
        """Main entry point for reviewing an entire document."""
        try:
            if self.verbose:
                print("Starting document review...")
            
            # Extract context from hierarchical summary if available
            paper_context = {
                'paper_summary': document_json.get('hierarchical_summary', {})
                    .get('document_summary', {})
                    .get('document_analysis', '')
            }
            
            # Review each section
            section_reviews = []
            for section in document_json.get('sections', []):
                review = self.review_section(section, paper_context=paper_context)
                section_reviews.append({
                    'section_type': section['section_type'],
                    'review': review
                })
            
            # Compile final review
            final_review = self.compile_final_review(section_reviews, paper_context)
            
            # Add reviews to document
            document_json['reviews'] = {
                'section_reviews': section_reviews,
                'final_review': final_review
            }
            
            return document_json
                
        except Exception as e:
            logger.error(f"Error reviewing document: {str(e)}")
            return document_json

    def review_section(self, section: dict, paper_context: dict | str = None) -> dict:
        """Review a single section from the document."""
        try:
            if self.verbose:
                print(f"Reviewing {section['section_type']} section...")
            
            # Extract section info
            section_text = section['text']
            section_type = section['section_type']
            
            # Convert string context to dict if needed
            if isinstance(paper_context, str):
                paper_context = {'paper_summary': paper_context}
            elif not isinstance(paper_context, dict):
                paper_context = {'paper_summary': ''}

            result = self.forward(section_text, section_type, paper_context)
            
            # Add match_strings from original section for reference
            review = result.review if hasattr(result, 'review') else self._create_empty_review()['review']
            review['original_match_strings'] = section.get('match_strings', {})
            
            return review
                
        except Exception as e:
            logger.error(f"Error reviewing section: {str(e)}")
            return self._create_empty_review()['review']

    def forward(self, section_text: str, section_type: str, paper_context: dict) -> dspy.Prediction:
        """Review a section using chain-of-thought reasoning."""
        if not section_text or not section_type:
            logger.warning("Empty input received")
            return self._create_empty_review()

        try:
            with dspy.settings.context(lm=self.engine):
                result = self.reviewer(
                    section_text=section_text,
                    section_type=section_type,
                    context=paper_context.get('paper_summary', '')
                )
                
                # Validate and filter review items based on section type
                items = json.loads(result.review_items) if isinstance(result.review_items, str) else result.review_items
                filtered_items = self._validate_section_specific_feedback(items, section_type.lower())
                
                review_data = {
                    'match_strings': [],
                    'comments': [],
                    'revisions': [],
                    'initial_analysis': result.initial_analysis,
                    'reflection': result.reflection
                }
                
                for item in filtered_items:
                    review_data['match_strings'].append(item.get('match_text', ''))
                    review_data['comments'].append(item.get('comment', 'N/A'))
                    review_data['revisions'].append(item.get('revision', 'N/A'))
                
                return dspy.Prediction(review=review_data)
                
        except Exception as e:
            logger.error(f"Error in forward method: {str(e)}")
            return self._create_empty_review()

    def _validate_section_specific_feedback(self, review_items: list, section_type: str) -> list:
        """Filter review items based on section-specific criteria."""
        if section_type not in self.section_criteria:
            logger.warning(f"Unknown section type: {section_type}")
            return review_items
        
        criteria = self.section_criteria[section_type]
        filtered_items = []
        
        for item in review_items:
            comment = item.get('comment', '').lower()
            revision = item.get('revision', '').lower()
            
            # Check if comment contains any terms to avoid
            should_avoid = any(avoid_term in comment or avoid_term in revision 
                             for avoid_term in criteria['avoid'])
            
            # Check if comment contains focus terms
            has_focus = any(focus_term in comment or focus_term in revision 
                           for focus_term in criteria['focus'])
            
            if has_focus and not should_avoid:
                filtered_items.append(item)
            else:
                logger.debug(f"Filtered out review item for {section_type}: {item}")
        
        return filtered_items

    #TODO: add validation to increase depth and usefulness of feedback
    # and avoid superficial comments (this is like an internal reflection loop)
    # to improve itself
    def _validate_scientific_depth(self, result) -> bool:
        return True

    def _create_empty_review(self) -> dspy.Prediction:
        return dspy.Prediction(review={'match_strings': [], 'comments': [], 'revisions': []})

    def compile_final_review(self, section_analyses: list, paper_context: dict) -> dict:
        """Compile a final high-level review synthesizing all sections."""
        try:
            with dspy.settings.context(lm=self.engine):
                final_review = self.final_reviewer(
                    section_analyses=section_analyses,
                    paper_context=paper_context
                )
                
                return {
                    'overall_assessment': str(final_review.overall_assessment),
                    'key_strengths': [str(s) for s in (final_review.key_strengths if isinstance(final_review.key_strengths, list) else [final_review.key_strengths])],
                    'key_weaknesses': [str(w) for w in (final_review.key_weaknesses if isinstance(final_review.key_weaknesses, list) else [final_review.key_weaknesses])],
                    'recommendations': [str(r) for r in (final_review.recommendations if isinstance(final_review.recommendations, list) else [final_review.recommendations])],
                }
        except Exception as e:
            logger.error(f"Error compiling final review: {str(e)}")
            return {
                'overall_assessment': "Error generating final review",
                'key_strengths': [],
                'key_weaknesses': [],
                'recommendations': [],
            }


if __name__ == "__main__":
    import os
    import json
    
    # Test the reviewer with DSPy's LM directly
    lm = dspy.LM(
        'openrouter/openai/gpt-4o',
        api_base="https://openrouter.ai/api/v1",
        api_key=os.getenv("OPENROUTER_API_KEY"),
        temperature=0.7,
    )
    
    reviewer = Reviewer(
        engine=lm,
        reviewer_class="Predict",
        verbose=True,
    )
    
    # Load and process sample document
    with open("ref/sample_output.json", "r") as f:
        document = json.load(f)
    
    # Review entire document
    reviewed_document = reviewer.review_document(document)
    
    # Print results
    print(json.dumps(reviewed_document, indent=4))
